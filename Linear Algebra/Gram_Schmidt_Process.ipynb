{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gram-Schmidt process\n",
    "\n",
    "### Matrices in Python\n",
    "Remember the structure for matrices in *numpy* is,\n",
    "```python\n",
    "A[0, 0]  A[0, 1]  A[0, 2]  A[0, 3]\n",
    "A[1, 0]  A[1, 1]  A[1, 2]  A[1, 3]\n",
    "A[2, 0]  A[2, 1]  A[2, 2]  A[2, 3]\n",
    "A[3, 0]  A[3, 1]  A[3, 2]  A[3, 3]\n",
    "```\n",
    "You can access the value of each element individually using,\n",
    "```python\n",
    "A[n, m]\n",
    "```\n",
    "You can also access a whole row at a time using,\n",
    "```python\n",
    "A[n]\n",
    "```\n",
    "\n",
    "Building on last assignment, in this exercise you will need to select whole columns at a time.\n",
    "This can be done with,\n",
    "```python\n",
    "A[:, m]\n",
    "```\n",
    "which will select the m'th column (starting at zero).\n",
    "\n",
    "Take the dot product between vectors. This can be done using the @ operator.\n",
    "To dot product vectors u and v, use the code,\n",
    "```python\n",
    "u @ v\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "verySmallNumber = 1e-14 # That's 1×10⁻¹⁴ = 0.00000000000001\n",
    "\n",
    "# Our first function will perform the Gram-Schmidt procedure for 4 basis vectors.\n",
    "# We'll take this list of vectors as the columns of a matrix, A.\n",
    "# We'll then go through the vectors one at a time and set them to be orthogonal\n",
    "# to all the vectors that came before it. Before normalising.\n",
    "def gsBasis4(A) :\n",
    "    # Make B as a copy of A, since we're going to alter it's values.\n",
    "    B = np.array(A, dtype=np.float_) \n",
    "    \n",
    "    ############################ Col Zero ###############################################\n",
    "    # The zeroth column is easy, since it has no other vectors to make it normal to.\n",
    "    # All that needs to be done is to normalise it. I.e. divide by its modulus, or norm.\n",
    "    B[:, 0] = B[:, 0] / la.norm(B[:, 0])\n",
    "\n",
    "    \n",
    "    ############################ Col One ###############################################\n",
    "    # For the first column, we need to subtract any overlap with our new zeroth vector.\n",
    "    B[:, 1] = B[:, 1] - B[:, 1] @ B[:, 0] * B[:, 0]\n",
    "\n",
    "    # If there's anything left after that subtraction, then B[:, 1] is linearly independant of B[:, 0]\n",
    "    # If this is the case, we can normalise it. Otherwise we'll set that vector to zero.\n",
    "    if la.norm(B[:, 1]) > verySmallNumber :\n",
    "        B[:, 1] = B[:, 1] / la.norm(B[:, 1])\n",
    "    else :\n",
    "        B[:, 1] = np.zeros_like(B[:, 1])\n",
    "\n",
    "        \n",
    "    ############################ Col Two ###############################################\n",
    "    # Now we need to repeat the process for column 2.\n",
    "    # Insert two lines of code, the first to subtract the overlap with the zeroth vector,\n",
    "    # and the second to subtract the overlap with the first.\n",
    "    B[:, 2] = B[:, 2] - B[:, 2] @ B[:, 0] * B[:, 0]\n",
    "    B[:, 2] = B[:, 2] - B[:, 2] @ B[:, 1] * B[:, 1]\n",
    "    \n",
    "    # Again we'll need to normalise our new vector.\n",
    "    if la.norm(B[:, 2]) > verySmallNumber :\n",
    "        B[:, 2] = B[:, 2] / la.norm(B[:, 2])\n",
    "    else :\n",
    "        B[:, 2] = np.zeros_like(B[:, 2])\n",
    "    \n",
    "\n",
    "    ############################ Col Three ###############################################\n",
    "    # Finally, column three:\n",
    "    # Subtract the overlap with the first three vectors.\n",
    "    B[:, 3] = B[:, 3] - B[:, 3] @ B[:, 0] * B[:, 0]\n",
    "    B[:, 3] = B[:, 3] - B[:, 3] @ B[:, 1] * B[:, 1]\n",
    "    B[:, 3] = B[:, 3] - B[:, 3] @ B[:, 2] * B[:, 2]\n",
    "    \n",
    "    # Now normalise\n",
    "    if la.norm(B[:, 3]) > verySmallNumber :\n",
    "        B[:, 3] = B[:, 3] / la.norm(B[:, 3])\n",
    "    else :\n",
    "        B[:, 3] = np.zeros_like(B[:, 3])\n",
    "    \n",
    "    \n",
    "    # Finally, we return the result:\n",
    "    return B\n",
    "\n",
    "\n",
    "# The second part of this exercise will generalise the procedure.\n",
    "# Previously, we could only have four vectors, and there was a lot of repeating in the code.\n",
    "# We'll use a for-loop here to iterate the process for each vector.\n",
    "def gsBasis(A) :\n",
    "    # Make B as a copy of A, since we're going to alter it's values.\n",
    "    B = np.array(A, dtype=np.float_) \n",
    "\n",
    "    # Loop over all vectors, starting with zero, label them with i\n",
    "    for i in range(B.shape[1]) :\n",
    "        # Inside that loop, loop over all previous vectors, j, to subtract.\n",
    "        for j in range(i) :\n",
    "            # Subtract the overlap with previous vectors.\n",
    "            # you'll need the current vector B[:, i] and a previous vector B[:, j]\n",
    "            B[:, i] = B[:, i] - B[:, i] @ B[:, j] * B[:, j]\n",
    "    \n",
    "        # Normalisation test for B[:, i]\n",
    "        if la.norm(B[:, i]) > verySmallNumber :\n",
    "            B[:, i] = B[:, i] / la.norm(B[:, i])\n",
    "        else :\n",
    "            B[:, i] = np.zeros_like(B[:, i])\n",
    "            \n",
    "    # Finally, we return the result:\n",
    "    return B\n",
    "\n",
    "# This function uses the Gram-schmidt process to calculate the dimension\n",
    "# spanned by a list of vectors.\n",
    "# Since each vector is normalised to one, or is zero,\n",
    "# the sum of all the norms will be the dimension.\n",
    "def dimensions(A) :\n",
    "    return np.sum(la.norm(gsBasis(A), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.1814885 ,  0.04982278,  0.89325973],\n",
       "       [ 0.        ,  0.1088931 ,  0.99349591, -0.03328918],\n",
       "       [ 0.81649658,  0.50816781, -0.06462163, -0.26631346],\n",
       "       [ 0.40824829, -0.83484711,  0.07942048, -0.36063281]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = np.array([[1,0,2,6],\n",
    "              [0,1,8,2],\n",
    "              [2,8,3,1],\n",
    "              [1,-6,2,3]], dtype=np.float_)\n",
    "gsBasis4(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.1814885 ,  0.04982278,  0.89325973],\n",
       "       [ 0.        ,  0.1088931 ,  0.99349591, -0.03328918],\n",
       "       [ 0.81649658,  0.50816781, -0.06462163, -0.26631346],\n",
       "       [ 0.40824829, -0.83484711,  0.07942048, -0.36063281]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once you've done Gram-Schmidt once,\n",
    "# doing it again should give you the same result. Test this:\n",
    "U = gsBasis4(V)\n",
    "gsBasis4(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.1814885 ,  0.04982278,  0.89325973],\n",
       "       [ 0.        ,  0.1088931 ,  0.99349591, -0.03328918],\n",
       "       [ 0.81649658,  0.50816781, -0.06462163, -0.26631346],\n",
       "       [ 0.40824829, -0.83484711,  0.07942048, -0.36063281]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try the general function too.\n",
    "gsBasis(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23643312,  0.18771349,  0.22132104],\n",
       "       [ 0.15762208,  0.74769023, -0.64395812],\n",
       "       [ 0.15762208,  0.57790444,  0.72904263],\n",
       "       [ 0.94573249, -0.26786082, -0.06951101]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what happens for non-square matrices\n",
    "A = np.array([[3,2,3],\n",
    "              [2,5,-1],\n",
    "              [2,4,8],\n",
    "              [12,2,1]], dtype=np.float_)\n",
    "gsBasis(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.93704257, -0.12700832, -0.32530002,  0.        ,  0.        ],\n",
       "       [ 0.31234752,  0.72140727,  0.61807005,  0.        ,  0.        ],\n",
       "       [ 0.15617376, -0.6807646 ,  0.71566005,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([[6,2,1,7,5],\n",
    "              [2,8,5,-4,1],\n",
    "              [1,-6,3,2,8]], dtype=np.float_)\n",
    "gsBasis(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678,  0.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ,  0.        ],\n",
       "       [ 0.70710678,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's see what happens when we have one vector that is a linear combination of the others.\n",
    "C = np.array([[1,0,2],\n",
    "              [0,1,-3],\n",
    "              [1,0,2]], dtype=np.float_)\n",
    "gsBasis(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[-0.5548766,  0.3272594, -0.7648616, -0.613572 ],\n",
    "       [ 0.8070932,  0.4347383, -0.3995036,  0.6902685],\n",
    "       [ 0.2017733, -0.8389898, -0.505355 ,  0.3834825]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.55487662,  0.3272594 , -0.76486157,  0.        ],\n",
       "       [ 0.80709322,  0.4347383 , -0.39950361,  0.        ],\n",
       "       [ 0.20177331, -0.83898981, -0.50535496,  0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsBasis4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.55487662,  0.3272594 , -0.76486157,  0.        ],\n",
       "       [ 0.80709322,  0.4347383 , -0.39950361,  0.        ],\n",
       "       [ 0.20177331, -0.83898981, -0.50535496,  0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsBasis(x)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "linear-algebra-machine-learning",
   "graded_item_id": "FNk8v",
   "launcher_item_id": "DdvVk"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
